# Yolo-based-mask-recognition-algorithm
基于Yolo的口罩识别算法研究与实现
 
![img](file:///C:/Users/hpf19/AppData/Local/Temp/msohtmlclip1/01/clip_image001.gif)

​                        图1 基于YOLOv5的口罩识别实现流程

图1是基于YOLOv5实现本次毕业设计目标——实时检测人脸佩戴口罩情况的实现流程图。

首先，收集整理原始的图像数据，既包含佩戴口罩的样本，也包含未佩戴口罩的样本。然后经过样本标注，将原始数据加工处理，成为能够用于YOLOv5模型训练的数据集。

然后，搭建好实验平台，安装所需的硬件驱动与依赖库，配置YOLO的运行环境，将数据集输入到卷积神经网络中进行训练。

最后，导入测试数据对训练结果进行测试。配置检测方法，调用训练好的模型针对图像集、视频、IP摄像头等图像源进行实时检测。

1 数据集

机器学习中的数据集可以分为三种，分别是训练集train、验证集valid和测试集test。其中，训练集作为输入卷积神经网络的原始图像数据，用于网络的训练；验证集用于在训练过程当中校验模型的状态与收敛情况，辅助训练参数的更新，也能通过模型针对它的检测精度来判断何时停止训练；测试集不在训练过程中输入网络，用于测试训练完成的模型对未见过的数据的检测能力。

本次毕业设计所用的数据集是来自AIZOO的开源数据集，原数据集大小为776MB，其中，训练集包含6120张图片，验证集包含1839张图片，图片已经经过标注。标注格式为VOC格式,即每个数据集都有两个文件夹，images文件夹中存放图片，labels文件夹中存放与images文件夹中图片一一对应的同名标注文本文件。如图4.2数据集文件的例子所示，其中每行第一个数字为是否佩戴口罩，佩戴口罩取1，未佩戴口罩取0；第二和第三个数字为目标边界框的中心横纵坐标；第四和第五个数字表示目标边界框的宽和高。如本文3.2.1中提到的，这里的坐标也根据图片尺度进行了处理。

![img](file:///C:/Users/hpf19/AppData/Local/Temp/msohtmlclip1/01/clip_image003.jpg)

图2 数据集文件

为了避免错误的图片和数量过多的单一类型图片，在实验开始前对数据集进行了手工筛选，最终使用留出法进行划分，即让各个数据集中没有重复部分。整理得到的数据集为：2000张图片的训练集，300张图片的验证集，300张图片的测试集。

2 实验

2.1实验平台搭建

本次毕业设计的训练平台为Google Colaboratory[34]。由于本地计算机性能较差， 显存也并不足以在合适的训练参数下装入训练数据，故采用Colaboratory平台完成训练，该平台本质上是一个云端Ubuntu环境，运行Jupyter notebook，由谷歌提供GPU帮助用户进行深度学习训练。

由于Colaboratory通过云端服务器进行训练，必须先把训练所需的数据集上传到Google的云端硬盘中，为了节约时间，可以本地压缩后再进行上传。

首先在平台中新建笔记本，并在笔记本设置中将硬件加速器设置为GPU，重新连接后就获得了云端的GPU算力。然后根据官方放出的requirement.txt文件，通过pip方式安装训练所需的包。接着将笔记本与云端硬盘进行连接，使用unzip解压上传的数据集，就能像在本地一样完成其他操作。

平台提供的软硬件环境如下：

硬件环境：

（1）  CPU：Intel(R) Xeon(R) CPU @ 2.20GHz

（2）  GPU：Tesla V100-SXM2-16GB

（3）  内存：12GB

软件环境：

（1）  开发语言：Python 3.7.10

（2）  操作系统：Ubuntu 18.04.5 LTS

（3）  依赖库：PyTorch 1.8.1 + CUDA 10.2

2.2模型训练

新建训练配置文件data.yaml，写入训练集与验证集图像文件所在的images文件夹地址路径，修改nc即类别数为2，类名为no-mask和mask对应数据集中的0和1。

训练涉及的所有参数如图4.3的代码截图所示。包括weight权重文件，借此可以设置迁移学习的基础权重；cfg用于选择模型文件；data设置数据集的路径；epoch即训练的迭代轮数；batch-size为训练批次大小，也相当于更新权重文件的步长；img-size是图像缩放后的分辨率大小。

![文本  描述已自动生成](file:///C:/Users/hpf19/AppData/Local/Temp/msohtmlclip1/01/clip_image005.jpg)

图3 YOLOv5可自定义参数

本次训练设置训练迭代次数epoch为300次，batch-size为64，图像大小为![img](file:///C:/Users/hpf19/AppData/Local/Temp/msohtmlclip1/01/clip_image007.gif)，其他参数保留默认值。由于完整的重新训练耗时较长，本次训练采用迁移训练的方式，基于YOLOv5官方发布的yolov5s模型进行迁移训练。训练命令如图4.4截图所示。

![img](file:///C:/Users/hpf19/AppData/Local/Temp/msohtmlclip1/01/clip_image009.jpg)

图4 开始训练代码

在将数据集加载进显存后，YOLOv5将针对输入的训练集自动完成自适应锚定边界框的选取，训练过程输出如图5训练过程的输出图所示，占用大约11.5GB的显存，一次迭代大约需要一分钟。

![img](file:///C:/Users/hpf19/AppData/Local/Temp/msohtmlclip1/01/clip_image011.jpg)

图5 训练过程的输出

训练耗时5.781小时，随着迭代次数增长，各个损失函数的指标不断的更新优化。最终，如图4.6的PR曲线和表4.1所示，在验证集上获得了91.4%的mAP，其中，对未佩戴口罩识别正确率为85.8%，对已佩戴口罩识别的正确率为97.1%。

![图片包含 图表  描述已自动生成](file:///C:/Users/hpf19/AppData/Local/Temp/msohtmlclip1/01/clip_image013.gif)

图6 PR曲线

表1 完成训练的模型在验证集上的测试结果

| Class   | Images(张) | Labels(个) | P(%)  | R(%)  | mAP@.5(%) |
| ------- | ---------- | ---------- | ----- | ----- | --------- |
| all     | 300        | 735        | 0.935 | 0.888 | 0.914     |
| no-mask | 300        | 493        | 0.931 | 0.821 | 0.858     |
| mask    | 300        | 242        | 0.938 | 0.955 | 0.971     |

3测试与分析

本文第二章第5节已经对卷积神经网络模型的评价指标做了说明，下图4.7混淆矩阵是训练完成后自动生成的混淆矩阵图，可以看到模型针对已佩戴口罩对象的检测精度最高，达到了93%，对未佩戴口罩的检出率也有81%。

测试中存在检测将background识别为人脸并进行口罩佩戴识别的情况，可能是yolov5更擅长检测大目标，针对单一种类的人脸这样较小的目标定位能力较差。

![img](file:///C:/Users/hpf19/AppData/Local/Temp/msohtmlclip1/01/clip_image015.gif)

图7 混淆矩阵

![img](file:///C:/Users/hpf19/AppData/Local/Temp/msohtmlclip1/01/clip_image017.gif)

图8 训练结果

如图8所示，迭代次数超过200时mAP基本已经稳定，损失函数也在逼近0，但验证集上关于目标位置的误差还是较大，这也符合上文对YOLOv5针对小目标定位性能较差的猜测。分类误差很低，说明人脸是否佩戴口罩的特征较为明显，针对定位到的人脸进行判断比较容易。

调用训练好的模型文件即可对不同的图像输入源进行实时检测，借此可以测试模型的训练质量和实际应用的能力。

测试平台的硬件配置如下：

（1）  CPU：Intel(R) Core (TM) i7-7700HQ CPU @ 2.80GHz

（2）  GPU：NVIDIA GeForce GTX 1050 4GB

（3）  内存：16GB

本地的硬件平台不同于训练所用的云端平台，需要自行配置许多运行环境。首先下载yolov5官网提供的Pytorch框架下实现源码，然后再NVIDIA官网下载硬件加速驱动CUDA10.2，接着在Pytorch官网根据自己的操作系统与算法语言获取相关依赖库版本，如图4.9所示。选择Windows和Python，选择通过pip方式安装，选择CUDA版本，就能获得依赖库的配置命令，在终端中输入命令进行配置。完成配置后就能在本地使用YOLO的全部功能。

![img](file:///C:/Users/hpf19/AppData/Local/Temp/msohtmlclip1/01/clip_image019.jpg)

图9 获取依赖库安装命令

主要测试模型对批量图片、视频的处理能力。由图10和图11所示，能够实时的检测出人类并对是否佩戴口罩进行较为可靠的判断，检测的速度约为20FPS，针对摄像头采集的数据，降低分辨率至![img](file:///C:/Users/hpf19/AppData/Local/Temp/msohtmlclip1/01/clip_image021.gif)能够达到接近60FPS。

![img](file:///C:/Users/hpf19/AppData/Local/Temp/msohtmlclip1/01/clip_image023.jpg)

图10 视频测试截图

![img](file:///C:/Users/hpf19/AppData/Local/Temp/msohtmlclip1/01/clip_image025.jpg)

图11 图片测试

从上图9可以看出，此次检测针对是否佩戴口罩的分类判断较为准确，置信度也都比较高，但对同一图像内的多个目标定位还不足，部分图像内的人脸没有被检测到，也就没有进行是否佩戴口罩的识别。推测是数据集中缺少人脸侧面图像数据，网络未能在训练中具备定位人侧脸的能力。

总结上述测试结果可以得出，训练的模型足以用于针对商场、学校等公共场所入口人群口罩佩戴的实时检测，能够替代人工完成检测任务。
